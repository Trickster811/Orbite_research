{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import flwr as fl\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import mutual_info_classif, SelectKBest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\n\n# Define the Anomaly Transformer model\nclass AnomalyTransformer(nn.Module):\n    def __init__(self, input_dim=15, hidden_dim=256, output_dim=1, num_layers=2):\n        super(AnomalyTransformer, self).__init__()\n        self.encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=8, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.dropout = nn.Dropout(0.3)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()  \n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.transformer_encoder(x)\n        x = self.fc2(x)\n        return self.sigmoid(x)\n\n# Load dataset function\ndef load_data():\n    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.1,random_state=43) \n    train_index = X_train.columns\n    train_index\n    mutual_info = mutual_info_classif(X_train, y_train)\n    mutual_info = pd.Series(mutual_info)\n    mutual_info.index = train_index\n    mutual_info.sort_values(ascending=False)\n    Select_features = SelectKBest(mutual_info_classif, k=30)\n    Select_features.fit(X_train, y_train)\n    train_index[Select_features.get_support()]\n    columns=['duration', 'protocol_type', 'service', 'flag', 'src_bytes',\n       'dst_bytes', 'wrong_fragment', 'hot', 'logged_in', 'num_compromised',\n       'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate']\n\n\n    X_train=X_train[columns]\n    X_test=X_test[columns]\n    scaler = StandardScaler()\n\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test) \n    \n    \n    x_train = torch.tensor(X_train, dtype=torch.float32)\n    y_train = torch.tensor(y_train.to_numpy(), dtype=torch.float32).unsqueeze(1)  \n    x_test = torch.tensor(X_test, dtype=torch.float32)\n    y_test = torch.tensor(y_test.to_numpy(), dtype=torch.float32).unsqueeze(1)  \n    \n    train_dataset = TensorDataset(x_train, y_train)\n    test_dataset = TensorDataset(x_test, y_test)\n        \n    return train_dataset, test_dataset\n\n# Define Focal Loss\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=3.3, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        inputs = inputs.view(-1)  \n        targets = targets.view(-1)  \n\n        \n        pt = inputs * targets + (1 - inputs) * (1 - targets)  \n        focal_weight = (1 - pt) ** self.gamma\n        loss = -self.alpha * focal_weight * (targets * torch.log(inputs + 1e-9) + (1 - targets) * torch.log(1 - inputs + 1e-9))\n\n        if self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum()\n        else:\n            return loss\n\n# FL Client with energy tracking\nclass FLClient(fl.client.NumPyClient):\n    def __init__(self, model, train_loader, test_loader, device):\n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.test_loader = test_loader\n        self.device = device\n        self.criterion = FocalLoss(alpha=0.25, gamma=3.3, reduction='mean')\n        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0005)\n        \n\n    \n    def get_parameters(self, config):\n        return [val.cpu().numpy() for val in self.model.state_dict().values()]\n\n    def set_parameters(self, parameters):\n        state_dict = {k: torch.tensor(v, dtype=torch.float32, device=self.device) for k, v in zip(self.model.state_dict().keys(), parameters)}\n        self.model.load_state_dict(state_dict, strict=True)\n\n    def fit(self, parameters, config):\n        self.set_parameters(parameters)\n        self.model.train()\n        \n        \n        for epoch in range(3):  \n            for x, y in self.train_loader:\n                x, y = x.to(self.device), y.to(self.device)\n                self.optimizer.zero_grad()\n                outputs = self.model(x)\n                loss = self.criterion(outputs, y)\n                loss.backward()\n                self.optimizer.step()\n        \n        return self.get_parameters({}), len(self.train_loader.dataset) \n\n    def evaluate(self, parameters, config):\n        self.set_parameters(parameters)\n        self.model.eval()\n\n        all_labels, all_preds, all_probs = [], [], []\n\n        with torch.no_grad():\n            for x, y in self.test_loader:\n                x, y = x.to(self.device), y.to(self.device)\n                outputs = self.model(x)\n                probs = outputs.cpu().numpy()\n                preds = (probs > 0.5).astype(int)\n\n                all_labels.append(y.cpu().numpy().flatten())\n                all_preds.append(preds.flatten())\n                all_probs.append(probs.flatten())\n        \n        all_labels = np.concatenate(all_labels, axis=0)\n        all_preds = np.concatenate(all_preds, axis=0)\n        all_probs = np.concatenate(all_probs, axis=0)\n\n        accuracy = accuracy_score(all_labels, all_preds)\n        precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n        recall = recall_score(all_labels, all_preds, average='binary', zero_division=0)\n        f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0)\n        auc_roc = roc_auc_score(all_labels, all_probs)\n\n        print(f\"Accuracy: {accuracy:.4f}\")\n        print(f\"Precision: {precision:.4f}\")\n        print(f\"Recall: {recall:.4f}\")\n        print(f\"F1 Score: {f1:.4f}\")\n        print(f\"AUC-ROC: {auc_roc:.4f}\")\n        return auc_roc, len(self.test_loader.dataset), {}\n\n# Client function\ndef client_fn(context):\n    train_dataset, test_dataset = load_data()\n    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n    model = AnomalyTransformer()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n   \n    return fl.client.NumPyClient.to_client(FLClient(model, train_loader, test_loader, device))\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}